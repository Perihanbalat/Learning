{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimisation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "A hyperparameter is any parameter whose value is used to control the learning process of a machine learning model, as opposed to a parameter which is learnt by the model during training. Hyperparameter optimization is the act of attempting to find a set of optimal hyperparameter values to get the best possible model performance. In this notebook, we will introduce two methods implemented in sklearn of performing hyperparameter optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use the 20 Newsgroups dataset for this section, the modelling task in this case is to classify a document as being about religon or athiesm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# downloading the data\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "]\n",
    "\n",
    "training_data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "X_train = training_data.data\n",
    "Y_train = training_data.target\n",
    "\n",
    "testing_data = fetch_20newsgroups(subset='test', categories=categories)\n",
    "X_test = testing_data.data\n",
    "Y_test = testing_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# we will first define a pipeline, where one of the steps is a custom transformer defined in the transformers and pipelines notebook\n",
    "class ToDenseTransformer(BaseEstimator,TransformerMixin):\n",
    "\n",
    "    # define the transform operation\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    # no paramter to learn this case\n",
    "    # fit just returns an unchanged object\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# initiate a pipeline object\n",
    "pipeline = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('to_dense', ToDenseTransformer()),\n",
    "    ('pca', PCA()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common method of hyperparameter optimisation is a grid search. For each hyperparameter of interest we define a set of values to test. We then exaustively train and evaulate our model using all of the possible combinations of hyperparameters. In sklearn we can grid search estimators and whole pipelines. Below is an example of grid searching the pipeline defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# first we need to define a parameter search grid.\n",
    "# Our parameter grid is a python dictionary or a list of python dictionaries. Each dictionary key is the name of a hyperparameter you want to tune, the corresponding value is a list of the desired options.\n",
    "\n",
    "# if you are grid searching a pipeline, the key must follow the following syntax 'pipelineStepName__hyperparameterName'. \n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'cv__max_features':[100,200,500,1000], \n",
    "        'pca__n_components':[2,5,10,20],\n",
    "        'clf__n_estimators':[200, 500]      \n",
    "    }\n",
    "]\n",
    "\n",
    "# we can also grid search the transformer or estimator choice itself \n",
    "# for example:\n",
    "# param_grid = [\n",
    "#     {\n",
    "#         'cv': [CountVectorizer(), TfidfVectorizer],\n",
    "#         'pca__n_components':[2,5,10,20],\n",
    "#         'clf': [RandomForestClassifier(), XGBoostClassifier()]     \n",
    "#     }\n",
    "# ]  \n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipeline,\n",
    "                    cv=3, # the number of folds for cross validation\n",
    "                    param_grid=param_grid,\n",
    "                    n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# to perform the grid search we call the fit method\n",
    "grid.fit(X_train,Y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__n_estimators': 200, 'cv__max_features': 1000, 'pca__n_components': 20}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to see the best hyperparameters\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68       319\n",
      "           1       0.59      0.56      0.57       251\n",
      "\n",
      "    accuracy                           0.63       570\n",
      "   macro avg       0.63      0.63      0.63       570\n",
      "weighted avg       0.63      0.63      0.63       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can make predictions using the best performing model using the predict method\n",
    "Y_pred = grid.predict(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "Grid searching suffers from the curse of dimensionality and the number of combinations to test can quickly become infeasible. Another type of hyperparameter optimisation that avoids this problem is a random search. Random search simply selects the value for a hyperparameter randomly from a set of distrete options, or a distribution, a predefined number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# define our parameter distributions. In this case we are treating each hyperparameter as uniform random discrete variables.\n",
    "param_dist = {\n",
    "        'cv__max_features':sp_randint(100, 3000),\n",
    "        'pca__n_components':sp_randint(1, 60),\n",
    "        'clf__n_estimators':sp_randint(50, 1000)\n",
    "    }\n",
    "\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(pipeline,\n",
    "                                   param_distributions=param_dist, \n",
    "                                   n_iter=n_iter_search, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# to perform the grid search we call the fit method\n",
    "random_search.fit(X_train,Y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.78      0.72       319\n",
      "           1       0.65      0.53      0.58       251\n",
      "\n",
      "    accuracy                           0.67       570\n",
      "   macro avg       0.66      0.65      0.65       570\n",
      "weighted avg       0.66      0.67      0.66       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred = random_search.predict(X_test)\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "There are other methods for hyperperamater optimisation not covered here, for example bayasian and genetic optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nteract": {
   "version": "0.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
